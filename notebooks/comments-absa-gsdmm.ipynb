{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data pre-processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# List of stop words used for data pre-processing\n",
    "from stop_words_list import stop_words_list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Topics modelling\n",
    "from gsdmm.gsdmm.mgp import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List of available creators: \"MKBHD\", \"Jonathan Morrison\", \"Unbox Therapy\", \"Chris Stuckman\", \"Jeremy Jahns\", \"Channel Awesome\", \"James Charles\", \"NikkieTutorials\", \"sophdoeslife\"\n",
    "\n",
    "creator = \"NikkieTutorials\"\n",
    "output_df = pd.read_excel(f\"../comments_spreadsheets/extracted_comments_{creator}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "duplicates = output_df[output_df.duplicated((\"Comments\"))]\n",
    "print (\"Count of duplicate comments in dataframe\"\n",
    ", duplicates.shape[0])\n",
    "\n",
    "print (\"Count of unique comments in dataframe\"\n",
    ", output_df.shape[0] - duplicates.shape[0])\n",
    "\n",
    "# Remove duplicated comments from dataset\n",
    "unique_df = output_df.drop_duplicates(subset=[\"Comments\"], keep='first', ignore_index=True)\n",
    "df = unique_df\n",
    "\n",
    "# Removes line return \"\\n\"\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sentimentAnalyser = SentimentIntensityAnalyzer()\n",
    "sentimentScoreList = []\n",
    "sentimentLabelList = []\n",
    "\n",
    "for i in df[\"Comments\"].values.tolist():\n",
    "    sentimentScore = sentimentAnalyser.polarity_scores(i)\n",
    "\n",
    "    if sentimentScore['compound'] >= 0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Positive')\n",
    "    elif sentimentScore['compound'] > -0.05 and sentimentScore['compound'] < 0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Neutral')\n",
    "    elif sentimentScore['compound'] <= -0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Negative')\n",
    "\n",
    "df[\"Sentiment\"] = sentimentLabelList\n",
    "df[\"Sentiment Score\"] = sentimentScoreList\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# Convert case text as lowercase, remove punctuation, remove extra whitespace in string and on both sides of string\n",
    "# Lowercase all the letters\n",
    "df['lower'] = df['Comments'].str.lower()\n",
    "\n",
    "# Remove punctuations\n",
    "df['punctuation_removed'] = df['lower'].str.replace(\"'\", '', regex=True).str.replace('[^\\w\\s]', ' ', regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "df['numbers_removed'] = df['punctuation_removed'].str.replace(\" \\d+\", \" \", regex=True)\n",
    "\n",
    "# Remove extra whitespace\n",
    "df['extra_spaces_removed'] = df['numbers_removed'].str.replace(' +', ' ', regex=True).str.strip()\n",
    "\n",
    "# Tokenise\n",
    "df['tokenised'] = df.apply(lambda row: nltk.word_tokenize(row['extra_spaces_removed']), axis=1)\n",
    "\n",
    "# Stop words removal\n",
    "# Initiate stopwords from nltk\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Add additional missing terms\n",
    "stop_words.extend(stop_words_list)\n",
    "\n",
    "# Remove stopwords\n",
    "df['removed_stopwords'] = df['tokenised'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "docs = []\n",
    "\n",
    "for token in df.removed_stopwords:\n",
    "    docs.append(token)\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mgp = MovieGroupProcess(K=3, alpha=0.1, beta=1, n_iters=30)\n",
    "\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "\n",
    "mgp_output = mgp.fit(docs, n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Indices of most important clusters (by number of docs inside):', top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Returns the top words present in each topic cluster.\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Show the top 7 words in term frequency for each cluster\n",
    "top_words(mgp.cluster_word_distribution, top_index, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "topic_names = ['Topic #1', 'Topic #2', 'Topic #3']\n",
    "for i, topic_num in enumerate(top_index):\n",
    "    topic_dict[topic_num]=topic_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# index names\n",
    "docnames = ['Doc' + str(i) for i in range(len(df))]\n",
    "\n",
    "# Create a dataframe with the original text and its assigned topic.\n",
    "def create_topics_dataframe(data_text=df.Comments,  mgp=mgp, threshold=0.3, topic_dict=topic_dict, stem_text=docs):\n",
    "    result = pd.DataFrame(columns=['topic'])\n",
    "    for i, text in enumerate(data_text):\n",
    "        prob = mgp.choose_best_label(stem_text[i])\n",
    "        if prob[1] >= threshold:\n",
    "            result.at[i, 'topic'] = topic_dict[prob[0]]\n",
    "        else:\n",
    "            result.at[i, 'topic'] = 'Other'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dfx = create_topics_dataframe(data_text=df.Comments,  mgp=mgp, threshold=0.3, topic_dict=topic_dict, stem_text=docs)\n",
    "dfx.topic.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, dfx, left_index = True, right_index = True, how = 'outer')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results = df.groupby(['topic', 'Sentiment']).count().reset_index()\n",
    "\n",
    "graph_results = results[['topic', 'Sentiment', 'Sentiment Score']]\n",
    "graph_results = graph_results.pivot(index='topic', columns='Sentiment', values='Sentiment Score').reset_index()\n",
    "\n",
    "graph_results.set_index('topic', inplace=True)\n",
    "\n",
    "display(graph_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = graph_results.plot.bar(rot=90, figsize=(10,10))\n",
    "\n",
    "# Uncomment to save the figure as a png to current directory\n",
    "# fig.figure.savefig(f'{creator}_absa_gsdmm.png' , bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}