{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data pre-processing\n",
    "import nltk\n",
    "# List of stop words used for data pre-processing\n",
    "from stop_words_list import stop_words_list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Topics modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# listOfCreator: \"MKBHD\", \"Jonathan Morrison\", \"Unbox Therapy\", \"Chris Stuckman\", \"Jeremy Jahns\", \"Channel Awesome\", \"James Charles\", \"NikkieTutorials\", \"sophdoeslife\"\n",
    "\n",
    "creator = \"MKBHD\"\n",
    "output_df = pd.read_excel(f\"./comments_spreadsheets/extracted_comments_{creator}.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicates = output_df[output_df.duplicated((\"Comments\"))]\n",
    "print (\"Count of duplicate comments in dataframe\"\n",
    ", duplicates.shape[0])\n",
    "\n",
    "print (\"Count of unique comments in dataframe\"\n",
    ", output_df.shape[0] - duplicates.shape[0])\n",
    "\n",
    "# Remove duplicated comments from dataset\n",
    "unique_df = output_df.drop_duplicates(subset=[\"Comments\"], keep='first')\n",
    "# unique_df.to_excel(excel_writer=r\"./extracted_comments_MKBHD_Samsung_Galaxy_Review.xlsx\", index=False, header=True)\n",
    "# df = pd.read_excel(\"extracted_comments_MKBHD_Samsung_Galaxy_Review.xlsx\")\n",
    "df = unique_df.reset_index()\n",
    "\n",
    "# Removes line return \"\\n\"\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "\n",
    "# display(unique_df)\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentimentAnalyser = SentimentIntensityAnalyzer()\n",
    "sentimentScoreList = []\n",
    "sentimentLabelList = []\n",
    "\n",
    "for i in df[\"Comments\"].values.tolist():\n",
    "    sentimentScore = sentimentAnalyser.polarity_scores(i)\n",
    "\n",
    "    if sentimentScore['compound'] >= 0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Positive')\n",
    "    elif sentimentScore['compound'] > -0.05 and sentimentScore['compound'] < 0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Neutral')\n",
    "    elif sentimentScore['compound'] <= -0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Negative')\n",
    "\n",
    "df[\"Sentiment\"] = sentimentLabelList\n",
    "df[\"Sentiment Score\"] = sentimentScoreList\n",
    "\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Pre-Processing for LDA Model\n",
    "1. Lowercase the letters\n",
    "2. Remove the punctuation\n",
    "3. Remove the number\n",
    "4. Remove extra whitespaces\n",
    "5. Tokenisation\n",
    "6. Remove the stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def lemmatize_text(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# Convert case text as lowercase, remove punctuation, remove extra whitespace in string and on both sides of string\n",
    "# Lowercase all the letters\n",
    "df['lower'] = df['Comments'].str.lower()\n",
    "\n",
    "# Remove punctuations\n",
    "df['punctuation_removed'] = df['lower'].str.replace(\"'\", '', regex=True).str.replace('[^\\w\\s]', ' ', regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "df['numbers_removed'] = df['punctuation_removed'].str.replace(\" \\d+\", \" \", regex=True)\n",
    "\n",
    "# Remove extra whitespace\n",
    "df['extra_spaces_removed'] = df['numbers_removed'].str.replace(' +', ' ', regex=True).str.strip()\n",
    "\n",
    "# Tokenise\n",
    "df['tokenised'] = df.apply(lambda row: nltk.word_tokenize(row['extra_spaces_removed']), axis=1)\n",
    "\n",
    "# Stop words removal\n",
    "# initiate stopwords from nltk\n",
    "stop_words = stopwords.words('english')\n",
    "# add additional missing terms\n",
    "stop_words.extend(stop_words_list)\n",
    "# remove stopwords\n",
    "df['removed_stopwords'] = df['tokenised'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "display(df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vectorisation for LDA model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialise the count vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "\n",
    "# join the processed data to be vectorised\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    vectors.append(\", \".join(row[\"removed_stopwords\"]))\n",
    "\n",
    "vectorised = vectorizer.fit_transform(vectors)\n",
    "\n",
    "print(vectorised)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic Modelling Using LDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components = 3, # number of topics\n",
    "                                      random_state = 1,     # random state USED TO BE 10\n",
    "                                      evaluate_every = -1,  # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,         # Use all available CPUs\n",
    "                                 )\n",
    "\n",
    "# Returns a list of probability of belonging to each topic for each comment\n",
    "lda_output = lda_model.fit_transform(vectorised)\n",
    "\n",
    "# Column names\n",
    "topic_names = [\"Topic\" + str(i) for i in range(1, lda_model.n_components + 1)]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns = topic_names)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = (np.argmax(df_document_topic.values, axis=1)+1)\n",
    "df_document_topic['Dominant_topic'] = dominant_topic\n",
    "\n",
    "# Join to original dataframes\n",
    "\n",
    "df = pd.merge(df, df_document_topic, left_index = True, right_index = True, how = 'outer')\n",
    "display(df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# index names\n",
    "docnames = ['Doc' + str(i) for i in range(len(df))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# display(df_document_topic)\n",
    "\n",
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topic_names\n",
    "\n",
    "# display(df_topic_keywords)\n",
    "\n",
    "df_topic_no = pd.DataFrame(df_topic_keywords.idxmax())\n",
    "df_scores = pd.DataFrame(df_topic_keywords.max())\n",
    "\n",
    "tmp = pd.merge(df_topic_no, df_scores, left_index=True, right_index=True)\n",
    "tmp.columns = ['topic', 'relevance_score']\n",
    "\n",
    "tmp[\"topic\"] =  tmp.topic.str.extract('(\\d+)')\n",
    "tmp[\"topic\"] = tmp[\"topic\"].astype(int)\n",
    "\n",
    "display(tmp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "\n",
    "for i in tmp['topic'].unique():\n",
    "    tmp_1 = tmp.loc[tmp['topic'] == i].reset_index()\n",
    "    tmp_1 = tmp_1.sort_values('relevance_score', ascending=False).head(1)\n",
    "\n",
    "    # print(tmp_1)\n",
    "    # print(tmp_1['topic'])\n",
    "    # tmp_1['topic'] = tmp_1['topic'] + 1\n",
    "\n",
    "    tmp_2 = []\n",
    "    tmp_2.append(tmp_1['topic'].unique()[0])\n",
    "    tmp_2.append(list(tmp_1['index'].unique()))\n",
    "    all_topics.append(tmp_2)\n",
    "\n",
    "all_topics = pd.DataFrame(all_topics, columns=['Dominant_topic', 'topic_name'])\n",
    "display(all_topics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = df.groupby(['Dominant_topic', 'Sentiment']).count().reset_index()\n",
    "# display(results)\n",
    "\n",
    "results = results.merge(all_topics, on='Dominant_topic')\n",
    "results['topic_name'] = results['topic_name'].apply(', '.join)\n",
    "\n",
    "# display(results)\n",
    "\n",
    "graph_results = results[['topic_name', 'Sentiment', 'Sentiment Score']]\n",
    "graph_results = graph_results.pivot(index='topic_name', columns='Sentiment', values='Sentiment Score').reset_index()\n",
    "\n",
    "graph_results.set_index('topic_name', inplace=True)\n",
    "display(graph_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = graph_results.plot.bar(rot=90, figsize=(10,10))\n",
    "\n",
    "# Uncomment to save the figure as a png to current directory\n",
    "# fig.figure.savefig(f'{creator}_absa_lda.png' , bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}