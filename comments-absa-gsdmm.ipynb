{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data pre-processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# List of stop words used for data pre-processing\n",
    "from stop_words_list import stop_words_list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Topics modelling\n",
    "from gsdmm.gsdmm.mgp import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'extracted_comments_sophdoeslife.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/yl/pr52dtxn48s9tqvcb70mkk340000gn/T/ipykernel_28462/597033325.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mcreator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"sophdoeslife\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0moutput_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"extracted_comments_{creator}.xlsx\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/envs/PRJ/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/PRJ/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001B[0m in \u001B[0;36mread_excel\u001B[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[1;32m    362\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    363\u001B[0m         \u001B[0mshould_close\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 364\u001B[0;31m         \u001B[0mio\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    365\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    366\u001B[0m         raise ValueError(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/PRJ/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[1;32m   1189\u001B[0m                 \u001B[0mext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"xls\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1190\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1191\u001B[0;31m                 ext = inspect_excel_format(\n\u001B[0m\u001B[1;32m   1192\u001B[0m                     \u001B[0mcontent_or_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1193\u001B[0m                 )\n",
      "\u001B[0;32m~/opt/anaconda3/envs/PRJ/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001B[0m in \u001B[0;36minspect_excel_format\u001B[0;34m(content_or_path, storage_options)\u001B[0m\n\u001B[1;32m   1068\u001B[0m         \u001B[0mcontent_or_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBytesIO\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcontent_or_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1069\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1070\u001B[0;31m     with get_handle(\n\u001B[0m\u001B[1;32m   1071\u001B[0m         \u001B[0mcontent_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_text\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1072\u001B[0m     ) as handle:\n",
      "\u001B[0;32m~/opt/anaconda3/envs/PRJ/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    709\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    710\u001B[0m             \u001B[0;31m# Binary mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 711\u001B[0;31m             \u001B[0mhandle\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    712\u001B[0m         \u001B[0mhandles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    713\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'extracted_comments_sophdoeslife.xlsx'"
     ]
    }
   ],
   "source": [
    "# listOfCreator: \"MKBHD\", \"Jonathan Morrison\", \"Unbox Therapy\", \"Chris Stuckman\", \"Jeremy Jahns\", \"Channel Awesome\", \"James Charles\", \"NikkieTutorials\", \"sophdoeslife\"\n",
    "\n",
    "creator = \"sophdoeslife\"\n",
    "output_df = pd.read_excel(f\"extracted_comments_{creator}.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicates = output_df[output_df.duplicated((\"Comments\"))]\n",
    "print (\"Count of duplicate comments in dataframe\"\n",
    ", duplicates.shape[0])\n",
    "\n",
    "print (\"Count of unique comments in dataframe\"\n",
    ", output_df.shape[0] - duplicates.shape[0])\n",
    "\n",
    "# Remove duplicated comments from dataset\n",
    "unique_df = output_df.drop_duplicates(subset=[\"Comments\"], keep='first')\n",
    "df = unique_df.reset_index()\n",
    "\n",
    "# Removes line return \"\\n\"\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SENTIMENT ANALYSIS\n",
    "\n",
    "sentimentAnalyser = SentimentIntensityAnalyzer()\n",
    "sentimentScoreList = []\n",
    "sentimentLabelList = []\n",
    "\n",
    "for i in df[\"Comments\"].values.tolist():\n",
    "    sentimentScore = sentimentAnalyser.polarity_scores(i)\n",
    "\n",
    "    if sentimentScore['compound'] >= 0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Positive')\n",
    "    elif sentimentScore['compound'] > -0.05 and sentimentScore['compound'] < 0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Neutral')\n",
    "    elif sentimentScore['compound'] <= -0.05:\n",
    "        sentimentScoreList.append(sentimentScore['compound'])\n",
    "        sentimentLabelList.append('Negative')\n",
    "\n",
    "df[\"Sentiment\"] = sentimentLabelList\n",
    "df[\"Sentiment Score\"] = sentimentScoreList\n",
    "\n",
    "display(df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# Convert case text as lowercase, remove punctuation, remove extra whitespace in string and on both sides of string\n",
    "# Lowercase all the letters\n",
    "df['lower'] = df['Comments'].str.lower()\n",
    "\n",
    "# Remove punctuations\n",
    "df['punctuation_removed'] = df['lower'].str.replace(\"'\", '', regex=True).str.replace('[^\\w\\s]', ' ', regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "df['numbers_removed'] = df['punctuation_removed'].str.replace(\" \\d+\", \" \", regex=True)\n",
    "\n",
    "# Remove extra whitespace\n",
    "df['extra_spaces_removed'] = df['numbers_removed'].str.replace(' +', ' ', regex=True).str.strip()\n",
    "\n",
    "# Tokenise\n",
    "df['tokenised'] = df.apply(lambda row: nltk.word_tokenize(row['extra_spaces_removed']), axis=1)\n",
    "\n",
    "# Stop words removal\n",
    "# initiate stopwords from nltk\n",
    "stop_words = stopwords.words('english')\n",
    "# add additional missing terms\n",
    "stop_words.extend(stop_words_list)\n",
    "# remove stopwords\n",
    "df['removed_stopwords'] = df['tokenised'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "docs = []\n",
    "\n",
    "for token in df.removed_stopwords:\n",
    "    docs.append(token)\n",
    "\n",
    "display(df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # DATA PRE-PROCESSING FOR GSDMM\n",
    "# stop_words = stopwords.words('english')\n",
    "#\n",
    "# def preprocess(comment):\n",
    "#     comment=str(comment)\n",
    "#     comment = comment.lower()\n",
    "#     comment=comment.replace('{html}',\"\")\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', comment)\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)\n",
    "#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stop_words]\n",
    "#     #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "#     return \" \".join(lemma_words)\n",
    "#\n",
    "#\n",
    "# # creating cleanText column for the processed data\n",
    "# df['cleanComments']=df['Comments'].map(lambda s:preprocess(s))\n",
    "#\n",
    "# # creating tokens column\n",
    "# df['tokens'] = df['cleanComments'].apply(word_tokenize)\n",
    "#\n",
    "# docs = []\n",
    "#\n",
    "# for token in df.tokens:\n",
    "#     docs.append(token)\n",
    "#\n",
    "# display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mgp = MovieGroupProcess(K=3, alpha=0.1, beta=1, n_iters=30)\n",
    "\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "\n",
    "mgp_output = mgp.fit(docs, n_terms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Indices of most important clusters (by number of docs inside):', top_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Returns the top words present in each topic cluster.\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        print('-'*120)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the top 7 words in term frequency for each cluster\n",
    "top_words(mgp.cluster_word_distribution, top_index, 7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "topic_names = ['Topic #1', 'Topic #2', 'Topic #3']\n",
    "for i, topic_num in enumerate(top_index):\n",
    "    topic_dict[topic_num]=topic_names[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# index names\n",
    "docnames = ['Doc' + str(i) for i in range(len(df))]\n",
    "\n",
    "# Create a dataframe with the original text and its assigned topic.\n",
    "def create_topics_dataframe(data_text=df.Comments,  mgp=mgp, threshold=0.3, topic_dict=topic_dict, stem_text=docs):\n",
    "    result = pd.DataFrame(columns=['topic'])\n",
    "    for i, text in enumerate(data_text):\n",
    "        prob = mgp.choose_best_label(stem_text[i])\n",
    "        if prob[1] >= threshold:\n",
    "            result.at[i, 'topic'] = topic_dict[prob[0]]\n",
    "        else:\n",
    "            result.at[i, 'topic'] = 'Other'\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfx = create_topics_dataframe(data_text=df.Comments,  mgp=mgp, threshold=0.3, topic_dict=topic_dict, stem_text=docs)\n",
    "dfx.topic.value_counts(dropna=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.merge(df, dfx, left_index = True, right_index = True, how = 'outer')\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = df.groupby(['topic', 'Sentiment']).count().reset_index()\n",
    "\n",
    "# display(results)\n",
    "\n",
    "# results = results.merge(all_topics, on='Dominant_topic')\n",
    "# results['topic_name'] = results['topic_name'].apply(', '.join)\n",
    "\n",
    "# display(results)\n",
    "\n",
    "graph_results = results[['topic', 'Sentiment', 'Sentiment Score']]\n",
    "graph_results = graph_results.pivot(index='topic', columns='Sentiment', values='Sentiment Score').reset_index()\n",
    "\n",
    "graph_results.set_index('topic', inplace=True)\n",
    "\n",
    "display(graph_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = graph_results.plot.bar(rot=90, figsize=(10,10))\n",
    "fig.figure.savefig(f'{creator}_absa_gsdmm.png' , bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}